{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDALab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oKv88s8QsneN",
        "3nqPtNfIs3Zi",
        "nX0Yv50Wh4nY",
        "Nt2Ty-GmcSY_",
        "3GqpbCcVdK83",
        "E3gBy530ektk",
        "EPECp0T6IU0Z",
        "MazFSZvFO9lT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bx2JSyCHfCq"
      },
      "source": [
        "# Lab 2: Color conversion in CUDA\n",
        "\n",
        "Please, follow the order of the sections, complete the code, and build a report that encapsulates everything you have learnt and understood."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Previous information\n"
      ],
      "metadata": {
        "id": "oKv88s8QsneN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "Following the previous laboratory based on OpenMP, now we are going to work on the same simple algorithm, in CUDA. The goal of this lab is to learn the basics of CUDA kernels, and CUDA Host code.\n",
        "\n",
        "This new ipython notebook format, will make things easyer since explanation and code will coexist in the same document, and it will be very clear what you need to do.\n",
        "\n",
        "Additionally, having the Colab platform available with NVIDIA GPU's makes it simpler than ever. You can learn CUDA from any system, Mac, Windows, Linux, and any hardware, Intel, AMD, NVIDIA, and possibliy even ARM on tablets. You only need a web browser compatible with Colab.\n"
      ],
      "metadata": {
        "id": "w4B3Aenesmcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Structure of the Lab\n",
        "You already know the algorithm from the previous lab, but you may not be familiar with this environment.\n",
        "\n",
        "First we will try to understand a bit this environment, and then we will explain section by section what you have to do. \n",
        "\n",
        "You will have to complete code  and perform experiments in each of the sections. Then, **you will be asked to comment the results in a separated report**:, just as you needed to do for the first lab. Use tables and figures that support both the results you collected and the arguments you make to justify the results.\n",
        "\n"
      ],
      "metadata": {
        "id": "T6wwlQsssj4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### The collab environment for CUDA\n",
        "\n",
        "First of all, you should know that we are executing an iPython notebook in a Google Colab session. The notebook is preconfigured with the type of execution environment we need, a GPU execution environment. But the files we generate, and the pluggins we install or enable, reside on the Google Colab session. All this will be removed when we exit the session either manually or implicitly by closing the broser.\n",
        "\n",
        "In order to have a GPU available when creating a new notebook, you only have to select the execution environment.\n",
        "In Spanish, go to \"Entorno de ejecución->Cambiar tipo de entorno de ejecución\" and then select GPU.\n",
        "\n",
        "But as we already mentioned, this notebook is already configured, so you don't need to do it again.\n",
        "\n",
        "Now, the first thing we will see is that we have the nvcc compiler. We can call many bash commands with ! as the first character, in a code block. Next you will find a code block with a call to nvcc (the nvidia CUDA compiler) with a flag that asks for the compiler version.\n",
        "\n",
        "Click on the block and then a play button will appear on the left. Click on the play button. \n",
        "\n"
      ],
      "metadata": {
        "id": "k_yqTZrbsemC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started..."
      ],
      "metadata": {
        "id": "3nqPtNfIs3Zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will check the CUDA compiler version we have:"
      ],
      "metadata": {
        "id": "Dp5rooWw0U_t"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNXYi-1xD-Zs"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4rhjdqSImo2"
      },
      "source": [
        "You can also execute it by placing the cursor inside the code block and pressing Shift+Enter\n",
        "\n",
        "Next you need to install a pluggin, that does not come with the notebook. In the following code block you have the code line to be executed. You will have to execute this code every time you open the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l1NOZW5ET_p"
      },
      "source": [
        "#!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO FOR GPU K80\n",
        "%cd /usr/local/\n",
        "!rm -rf cuda\n",
        "!ln -s /usr/local/cuda-10.1 /usr/local/cuda"
      ],
      "metadata": {
        "id": "yMdtAyAFz4L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuMz7OOYJVfk"
      },
      "source": [
        "Now, you can compile and execute CUDA code, just by puting the same code you would put ina .cu file, just by adding %%cu as the first line.\n",
        "\n",
        "Next, you have a code example. Try it! Read the comments to help you understand it. It will be very useful for the tasks you have to do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x03T7kcmEgu4"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// Function to print the cuda errors\n",
        "void cuCheck(cudaError_t err) {\n",
        "    if(err!=cudaSuccess) {\n",
        "          printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "}\n",
        "\n",
        "// This macros help with capturing the possible cuda errors and printing\n",
        "// the error name to help the developer.\n",
        "// Kernels are always asynchronous with respect to the Host, so they don't return\n",
        "// any value. Then to see if any error happened, you should call cudaGetLastError\n",
        "// and pass the result to cuCheck()\n",
        "// Use the macros instead, to make it simpler.\n",
        "#define CU_CHECK(a) cuCheck(a)\n",
        "#define CU_CHECK_LAST_ERROR cuCheck(cudaGetLastError())\n",
        "\n",
        "// Device code or Kernel\n",
        "__global__ void mult(int a, int b, int* __restrict d_c) {\n",
        "    *d_c = a * b;\n",
        "}\n",
        "\n",
        "// Host code\n",
        "int main() {\n",
        "    \n",
        "    // Host variables a & b\n",
        "    int a = 3, b = 5, h_c = 0;\n",
        "\n",
        "    // Host variable that will store a Device pointer wich we can later on \n",
        "    // download to the Host.\n",
        "    // As this variable will contain pointers that are only valid in\n",
        "    // the Device (the GPU) it will be invalid to access them from\n",
        "    // Host code. We only can use them in the right cuda API calls\n",
        "    // or inside a cuda Kernel.\n",
        "    // So in this part of the code you won't be able to do d_c[0], for instance\n",
        "    int *d_c;\n",
        "\n",
        "    // Size of the data contained in variables a, b and c.\n",
        "    int dataSize = sizeof(int);\n",
        "\n",
        "    // Reserve Device memory using the cuda API\n",
        "    // cudaMalloc will place a Device pointer inside d_c.\n",
        "    CU_CHECK(\n",
        "        cudaMalloc((void **)&d_c, dataSize)\n",
        "    );\n",
        "\n",
        "    // Launch mult() kernel on GPU\n",
        "    // Notice that a and b are not pointers. Therefore the kernel call will\n",
        "    // copy their values but the variables inside the kernel will not be the same.\n",
        "    // If we modify a and b inside the kernel, it will not change a and b in this\n",
        "    // Host code. This, indeed is the same behavior as any C/C++ function call.\n",
        "    // In the case of d_c, it will copy the pointer contained in d_c, \n",
        "    // so we will be able to modify the contents of d_c from the kernel. But to read \n",
        "    // them from this Host code, we will have to do something else.\n",
        "    mult<<<1,1>>>(a, b, d_c);\n",
        "    CU_CHECK_LAST_ERROR;\n",
        "\n",
        "    CU_CHECK(\n",
        "        // Copy result back to host\n",
        "        cudaMemcpy(&h_c, d_c, dataSize, cudaMemcpyDeviceToHost)\n",
        "    );\n",
        "\n",
        "    printf(\"Result of multiplying %d * %d is %d\\n\\n\",a,b,h_c);\n",
        "\n",
        "    int numDevs=0;\n",
        "    CU_CHECK(\n",
        "        cudaGetDeviceCount(&numDevs)\n",
        "    );\n",
        "\n",
        "    cudaDeviceProp prop;\n",
        "    CU_CHECK(\n",
        "        cudaGetDeviceProperties(&prop, 0)\n",
        "    );\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "          prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "          prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "          2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    printf(\"Num devices %d\\n\", numDevs);\n",
        "    // Cleanup\n",
        "    CU_CHECK(\n",
        "      cudaFree(d_c)\n",
        "    );\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> If your GPU is other than Tesla K80, tell the teacher please."
      ],
      "metadata": {
        "id": "5Y-tsavm1nzC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajyZJvBCZR7p"
      },
      "source": [
        "Ok, cool! But what if I want to have some code in a .h file, the cuda kernels in an other .h file, and include both so that I can reuse code?\n",
        "\n",
        "Ok, let's try to put the macros and cuCheck function in a .h file, the kernel in an other .h file and the rest in a .cu file, and compile and execute everything. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlObGBaEZ5M7"
      },
      "source": [
        "%%cuda --name utils.h\n",
        "#include <iostream>\n",
        "\n",
        "void cuCheck(cudaError_t err, const std::string message = \"CUDA error:\") {\n",
        "  if(err!=cudaSuccess) {\n",
        "    std::cout << message << \" ERROR \" << cudaGetErrorString(err) << std::endl;\n",
        "  }\n",
        "}\n",
        "#define CU_CHECK(a) cuCheck(a)\n",
        "#define CU_CHECK2(a, b) cuCheck(a, b)\n",
        "#define CU_CHECK_LAST_ERROR cuCheck(cudaGetLastError())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEoYz9uug7om"
      },
      "source": [
        "%%cuda --name kernels.h\n",
        "__global__ void mult(int a, int b, int* __restrict d_c) {\n",
        "    *d_c = a * b;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdHjgZRHlSGe"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/kernels.h\"\n",
        "\n",
        "// Host code\n",
        "int main() {\n",
        "    int a = 3, b = 5, h_c = 0;\n",
        "    int *d_c;\n",
        "    int dataSize = sizeof(int);\n",
        "    CU_CHECK(cudaMalloc((void **)&d_c, dataSize));\n",
        "    mult<<<1,1>>>(a, b, d_c);\n",
        "    CU_CHECK_LAST_ERROR;\n",
        "    CU_CHECK(cudaMemcpy(&h_c, d_c, dataSize, cudaMemcpyDeviceToHost));\n",
        "    int numDevs=0;\n",
        "    CU_CHECK(cudaGetDeviceCount(&numDevs));\n",
        "    cudaDeviceProp prop;\n",
        "    CU_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "          prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "          prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "          2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    printf(\"Num devices %d\\n\", numDevs);\n",
        "    printf(\"Result of multiplying %d * %d is %d\\n\",a,b,h_c);\n",
        "    CU_CHECK(cudaFree(d_c));\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8rWgfJ8hN3m"
      },
      "source": [
        "VERY IMPORTANT!!! On each Colab session, the GPU that Google Colab provides can be different. Take it into account when you perform experiments, so that you compare results for the same GPU.\n",
        "\n",
        "If you have to repeat all the experiments, well, it's not that hard, just click play in all the code blocks one by one.\n",
        "\n",
        "Great!! Now we can start the lab :-D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX0Yv50Wh4nY"
      },
      "source": [
        "##Section 1:\n",
        "\n",
        "Try to complete the following code, and make it compile. Remember that you have some slides and documents, and the CUDA API specification in the following link: https://docs.nvidia.com/cuda/cuda-runtime-api/index.html\n",
        "\n",
        "Also, you can search in Google, things like \"How to allocate CUDA memory\". And so on. Be brave! Is not so difficult.\n",
        "\n",
        "### First, complete the allocation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Bj54YdVsjI"
      },
      "source": [
        "%%cuda --name memory_functions.h\n",
        "void allocGPUData(int width, int height, uchar3** d_brg, uchar4** d_rgba){\n",
        "  // Alloc gpu pointers\n",
        "  CU_CHECK2(cudaMalloc(d_brg, sizeof(uchar3)*width*height), \"Alloc d_brg:\");\n",
        "  // Can you finish this one? Replace cud   aSucces with the proper cuda API call\n",
        "  CU_CHECK2(cudaSuccess, \"Alloc d_rgba:\");\n",
        "}\n",
        "void copyAndInitializeGPUData(int width, int height, uchar3* h_brg, uchar3* d_brg, uchar4* d_rgba, cudaStream_t stream=0) {\n",
        "  // Copy data to GPU\n",
        "  CU_CHECK2(cudaMemcpy(d_brg, h_brg, width*height*sizeof(uchar3), cudaMemcpyHostToDevice), \"Copy h_brg to d_brg:\");\n",
        "  // Init output buffer to 0\n",
        "  CU_CHECK2(cudaMemset(d_rgba, 0, width*height*sizeof(uchar4)), \"Memset d_rgba:\");\n",
        "}\n",
        "void freeCUDAPointers(uchar3* d_brg, uchar4* d_rgba) {\n",
        "  // Free cuda pointers. Replace the cudaErrorInvalidValue flag\n",
        "  // with the proper cuda API call, to free the GPU pointers\n",
        "  CU_CHECK2(cudaErrorInvalidValue, \"Cuda free d_bgr:\");\n",
        "  CU_CHECK2(cudaErrorInvalidValue, \"Cuda free d_rgba:\");\n",
        "  // Clean GPU device\n",
        "  CU_CHECK2(cudaDeviceReset(), \"Cuda device reset:\");\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s30mTBEIWcX6"
      },
      "source": [
        "### When completed, test that they work with this small main function. If you execute it without completing the previous code, it will show some errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tup9fxlwWl64"
      },
      "source": [
        "%%cu\n",
        "#include <cuda.h>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "\n",
        "#define WIDTH 10\n",
        "#define HEIGHT 10\n",
        "\n",
        "int main() {\n",
        "\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba);\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSfdiwiHZKPT"
      },
      "source": [
        "### Ok, now that we have the allocation, copy and free functions implemented, let's continue with the CPU function that will check the results. This one it's already implemented, you only need to click play to have it available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fywcbihaBaR"
      },
      "source": [
        "%%cuda --name check_results.h\n",
        "bool checkResults(uchar4* rgba, uchar3* brg, int size) {\n",
        "  bool correct = true;\n",
        "  for (int i=0; i < size; ++i) {\n",
        "    // In case you want to see actual values\n",
        "    if (i==3) {\n",
        "      unsigned char x, y, z, w;\n",
        "      x = rgba[i].x;\n",
        "      y = rgba[i].y;\n",
        "      z = rgba[i].z;\n",
        "      w = rgba[i].w;\n",
        "      std::cout << \"First position x=\" << (unsigned int)x << \" y=\" << (unsigned int)y << \" z=\" << (unsigned int)z << \" w=\" << (unsigned int)w << std::endl;\n",
        "    }\n",
        "    correct &= rgba[i].x == brg[i].y;\n",
        "    correct &= rgba[i].y == brg[i].z;\n",
        "    correct &= rgba[i].z == brg[i].x;\n",
        "    correct &= rgba[i].w == 255;\n",
        "  }\n",
        "  return correct;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl-YagCWaZOS"
      },
      "source": [
        "### Now the interesting part, the kernel and the code to configure and launch it. The kernel it's almost exactly the same code as the OpenMP lab, only we replaced the forloops with something that you need to implement.\n",
        "\n",
        "Remember, that we have threads with indexes. This indexes are used to tell each CUDA thread, which data do they have to read or write.\n",
        "\n",
        "The structs that contain those indexes are in the documentation you have available in campusvirtual. Please check the docs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et3Bww3PbMgw"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "// BIDIMENSIONAL KERNEL\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = 0;//Use the thread id and block id's to compute x \n",
        "  int y = 0;//Use the thread id and block id's to compute y\n",
        "\n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width && y < height) {\t\n",
        "\t    rgba[width * y + x].x = brg[width * y + x].y;\n",
        "\t    rgba[width * y + x].y = brg[width * y + x].z;\n",
        "\t    rgba[width * y + x].z = brg[width * y + x].x;\n",
        "\t    rgba[width * y + x].w = 255;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(256, 4, 1);\n",
        "  dim3 grid(ceil(width/(float)block.x),ceil(height/(float)block.y) , 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "\n",
        "  CU_CHECK2(cudaFree(d_secondData), \"cudaFree in executeKernelconvertBRG2RGBA\");\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt2Ty-GmcSY_"
      },
      "source": [
        "### MAIN EXPERIMENT \n",
        "Try all the previous code, with the following main. If you did not finish all the previous code, this file will show some execution errors.\n",
        "\n",
        "The code is divided in two parts, one to define the parameters of the experiment and the other one is the main function with the experiment it self.\n",
        "\n",
        "The experiment is the code that creates a BRG image in CPU, allocates GPU memory, copies the BRG image to GPU memory, and executes a GPU kernel to convert the BRG image into a RGBA image. The output of the kernel is another GPU pointer, so after the kernel execution, we have to copy back the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwcU52z8B--N"
      },
      "source": [
        "%%cuda --name experiment_settings.h\n",
        "#pragma once\n",
        "#define WIDTH 3840\n",
        "#define HEIGHT 2160\n",
        "#define EXPERIMENT_ITERATIONS 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT9ep3h_ijOO"
      },
      "source": [
        "%%cuda --name experiment.h\n",
        "#include <cuda.h>\n",
        "#include <chrono>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "#include \"/content/src/check_results.h\"\n",
        "#include \"/content/src/cuda_launcher.h\"\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "void executeExperiment() {\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  int bar_widht = (HEIGHT/3) * WIDTH;\n",
        "\n",
        "  // Alloc and generate BRG bars.\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  for (int i=0; i < WIDTH * HEIGHT; ++i) {\n",
        "    if (i < bar_widht) {\n",
        "      uchar3 temp = {255, 0, 0};\n",
        "      h_brg[i] = temp; \n",
        "    } else if (i < bar_widht*2) {\n",
        "      uchar3 temp = {0, 255, 0};\n",
        "      h_brg[i] = temp;\n",
        "    } else { \n",
        "      uchar3 temp = {0, 0, 255};\n",
        "      h_brg[i] = temp;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Alloc RGBA pointers\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  // Alloc gpu pointers\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "  \n",
        "  // Prepare and copy data to GPU\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba);\n",
        "\n",
        "  // Execute the GPU kernel\n",
        "  executeKernelconvertBRG2RGBA(WIDTH, HEIGHT, d_brg, d_rgba, EXPERIMENT_ITERATIONS);\n",
        "\n",
        "  // Copy data back from GPU to CPU, without streams\n",
        "  CU_CHECK2(cudaErrorInvalidValue, \"Cuda memcpy Device to Host: \");\n",
        "    \n",
        "  // Check results\n",
        "  bool ok = checkResults(h_rgba, h_brg, WIDTH*HEIGHT);\n",
        "  if (ok) {\n",
        "      std::cout << \"Executed!! Results OK.\" << std::endl;\n",
        "  } else {\n",
        "      std::cout << \"Executed!! Results NOT OK.\" << std::endl;\n",
        "  }\n",
        "\n",
        "  // Free CPU pointers\n",
        "  free(h_rgba);\n",
        "  free(h_brg);\n",
        "\n",
        "  // Free cuda pointers\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5RA9zDI7Z0V"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GqpbCcVdK83"
      },
      "source": [
        "##Section 2:\n",
        "Implement a version of the kernel and launcher that uses a one dimensional cuda GRID. That is, there is no more x and y, only x.\n",
        "\n",
        "Modify the code below, click play, and then click play in the Main Experiment block, in Section 1.\n",
        "\n",
        "Try different values of BLOCK_SIZE.\n",
        "\n",
        "Check if there is any execution time improvement, compared to Section 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLPY7trNd1SG"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = 0;//Use the thread id and block id's to compute x \n",
        "  \n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width * height) {\t\n",
        "\t    rgba[x].x = brg[x].y;\n",
        "\t    rgba[x].y = brg[x].z;\n",
        "\t    rgba[x].z = brg[x].x;\n",
        "\t    rgba[x].w = 255;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozsLq2rt__xl"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the WIDTH to 3839 and reexcecute section 1 and section 2. Compare the execution times and explain the difference."
      ],
      "metadata": {
        "id": "IKiFjB66Rp_j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExMCHf_RD3Wd"
      },
      "source": [
        "Change the experiment settings, by executing more iterations and compare the unidimensional kernel with the bidimensional kernel.\n",
        "\n",
        "Comment the results in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3gBy530ektk"
      },
      "source": [
        "## Section 3:\n",
        "\n",
        "Starting from Section 2, (use the best BLOCK_SIZE you found) try to optimize the memory accesses in some way, without using shared memory.\n",
        "\n",
        "Comment in the report which memory access problems you observe. Are the memory accesses aligned, and therfore coalesced?\n",
        "\n",
        "Remember that opposite to what the CPU compilers do, the nvcc compiler does not optimize the memory accesses in structs\n",
        "\n",
        "Remember also that GPU memory is organized in blocks of 4 bytes, and any array based on data elements that are not multiple of 2, will not be alligned. To be coalesced (specially in old architectures), it also has to be multiple of 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmP4PDZ-e4lG"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL BETTER MEMORY ACCESS\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = 0;//Use the same code as in section 2 in this line\n",
        "  \n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width * height) {\t\n",
        "\t    // do something different here, to optimize memory accesses\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMDGsQjPAkIv"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPECp0T6IU0Z"
      },
      "source": [
        "##Section 4:\n",
        "Now optimize the GPU memory accesses so that each thread always reads at least one element of 4 bytes. Use shared memory to that end.\n",
        "\n",
        "Look at the PDF Lab2CUDA in the campus, an read the last two pages. There you have a graphical explanation of the kernel issues. For this section you only need to understand the first figure.\n",
        "\n",
        "About shared memory: we will refresh some concepts.\n",
        "\n",
        "Shared memory, is a kind of memory that is visible only by the cuda threads of a thread block. Cuda threads from different thread blocks can not see the shared memory of other threadblocks.\n",
        "\n",
        "Shared memory is a limited resource. Depending on the GPU model, you may have from 32KB to 64KB of shared memory. Additionally, this memory is not used only by one threadblock. It is partitioned in as many independent blocks as thread blocks can execute in a single Streaming Multiprocessor (check the documentation if you don't know what a SM is). \n",
        "\n",
        "So when you are defining the amount of shared memory you want, you are defining the amount of memory, every thread block will have available.\n",
        "\n",
        "If you reserve 64KB of shared memory, in a GPU that has this capacity, only one thread block will execute on each SM, which is super slow. Each SM can concurrently execute from 8 to 32 thread blocks. For the best performance, you usually want the greatest amount of thread blocks active on each SM.\n",
        "\n",
        "Therefore, you what to use the least shared memory possible, and only use it when it has clear benefits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKRs22QeIvwj"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "// Try different vaues of BLOCK_SIZE\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Number of 4 byte elements that we can make out of BLOCK_SIZE elements of 3 bytes\n",
        "#define N_ELEMS_3_4_TBLOCK (BLOCK_SIZE * 3)/4\n",
        "#define N_ELEMS_3_4_IMAGE (WIDTH*HEIGHT * 3)/4\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL SHARED MEMORY\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int position = 0;// use N_ELEMS_3_4_TBLOCK to compute the position of each thread when we read brg as if it had elements of 4 bytes\n",
        "  __shared__ uchar4 bgrShared[N_ELEMS_3_4_TBLOCK];\n",
        "  \n",
        "  if(threadIdx.x < N_ELEMS_3_4_TBLOCK && position < N_ELEMS_3_4_IMAGE) {\n",
        "      uchar4* temp = reinterpret_cast<uchar4*>(brg);\n",
        "      bgrShared[threadIdx.x] = temp[position];\n",
        "  }\n",
        "\n",
        "  __syncthreads();\n",
        "  \n",
        "  position = 0;// recompute position without N_ELEMS_3_4_TBLOCK to write the results\n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (position < width*height) {\t\n",
        "        uchar3 local = reinterpret_cast<uchar3*>(bgrShared)[threadIdx.x];\n",
        "        rgba[position] = make_uchar4(local.y,local.z,local.x,255);\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E_ErYqLAtVU"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coment in the report how the time execution is changing."
      ],
      "metadata": {
        "id": "ZW8E-e-DVXk1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MazFSZvFO9lT"
      },
      "source": [
        "##Section 5:\n",
        "\n",
        "Change all the host code necessary, to use cuda streams. Here you have an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EvgoMT5PVYn"
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "\n",
        "__global__ void square(int* d_input, int* d_output) {\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    int val = d_input[x];\n",
        "    // We exploit the temporal locality of the value stored in d_output[x]\n",
        "    d_output[x] = val*val;\n",
        "}\n",
        "\n",
        "static const size_t dataSize = sizeof(int)*1024;\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    int *h_input, *h_output;\n",
        "    h_input = (int*)malloc(dataSize);\n",
        "    h_output = (int*)malloc(dataSize);\n",
        "\n",
        "    for (int i=0; i<1024; ++i) h_input[i]=i;\n",
        "\n",
        "    int *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, dataSize);\n",
        "    cudaMalloc(&d_output, dataSize);\n",
        "\n",
        "    cudaStream_t stream;\n",
        "    cudaStreamCreate(&stream);\n",
        "\n",
        "    dim3 block(512);\n",
        "    dim3 grid(2);\n",
        "\n",
        "    // The CPU thread does not wait that any of the following actions finish\n",
        "    // It only asks the GPU to do the copies and the kernel and continues\n",
        "    cudaMemcpyAsync(d_input, h_input, dataSize, cudaMemcpyHostToDevice, stream);\n",
        "    square<<<grid, block, 0, stream>>>(d_input, d_output);\n",
        "    cudaMemcpyAsync(h_output, d_output, dataSize, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    // Here, we wait for all the orders enqueued in stream, to finish.\n",
        "    cudaStreamSynchronize(stream);\n",
        "\n",
        "    bool correct = true;\n",
        "    for (int i=0; i<1024; ++i) correct &= h_output[i] == i*i;\n",
        "\n",
        "    std::cout << \"Finished and results are \" << (correct ? \"correct.\" : \"not correct.\") << std::endl;\n",
        "\n",
        "    cudaStreamDestroy(stream);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4VK6ylPWaEY"
      },
      "source": [
        "Modify this code, to use streams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAn_ggnrbAWq"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "// Try different vaues of BLOCK_SIZE\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Number of 4 byte elements that we can make out of BLOCK_SIZE elements of 3 bytes\n",
        "#define N_ELEMS_3_4_TBLOCK (BLOCK_SIZE * 3)/4\n",
        "#define N_ELEMS_3_4_IMAGE (WIDTH*HEIGHT * 3)/4\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL SHARED MEMORY\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int position = 0;// use N_ELEMS_3_4_TBLOCK to compute the position of each thread when we read brg as if it had elements of 4 bytes\n",
        "  __shared__ uchar4 bgrShared[N_ELEMS_3_4_TBLOCK];\n",
        "  \n",
        "  if(threadIdx.x < N_ELEMS_3_4_TBLOCK && position < N_ELEMS_3_4_IMAGE) {\n",
        "      uchar4* temp = reinterpret_cast<uchar4*>(brg);\n",
        "      bgrShared[threadIdx.x] = temp[position];\n",
        "  }\n",
        "\n",
        "  __syncthreads();\n",
        "  \n",
        "  position = 0;// recompute position without N_ELEMS_3_4_TBLOCK to write the results\n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (position < width*height) {\t\n",
        "        uchar3 local = reinterpret_cast<uchar3*>(bgrShared)[threadIdx.x];\n",
        "        rgba[position] = make_uchar4(local.y,local.z,local.x,255);\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz3kVFkrWOlB"
      },
      "source": [
        "%%cuda --name memory_functions.h\n",
        "void allocGPUData(int width, int height, uchar3** d_brg, uchar4** d_rgba){\n",
        "  // Alloc gpu pointers\n",
        "  CU_CHECK2(cudaMalloc(d_brg, sizeof(uchar3)*width*height), \"Alloc d_brg:\");\n",
        "  // Can you finish this one? Replace cudaSucces with the proper cuda API call\n",
        "  CU_CHECK2(cudaSuccess, \"Alloc d_rgba:\");\n",
        "}\n",
        "void copyAndInitializeGPUData(int width, int height, uchar3* h_brg, uchar3* d_brg, uchar4* d_rgba, cudaStream_t stream=0) {\n",
        "  // Copy data to GPU\n",
        "  CU_CHECK2(cudaMemcpy(d_brg, h_brg, width*height*sizeof(uchar3), cudaMemcpyHostToDevice), \"Copy h_brg to d_brg:\");\n",
        "  // Init output buffer to 0\n",
        "  CU_CHECK2(cudaMemset(d_rgba, 0, width*height*sizeof(uchar4)), \"Memset d_rgba:\");\n",
        "}\n",
        "void freeCUDAPointers(uchar3* d_brg, uchar4* d_rgba) {\n",
        "  // Free cuda pointers. Replace the cudaErrorInvalidValue flag\n",
        "  // with the proper cuda API call, to free the GPU pointers\n",
        "  CU_CHECK2(cudaErrorInvalidValue, \"Cuda free d_bgr:\");\n",
        "  CU_CHECK2(cudaErrorInvalidValue, \"Cuda free d_rgba:\");\n",
        "  // Clean GPU device\n",
        "  CU_CHECK2(cudaDeviceReset(), \"Cuda device reset:\");\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymPFWGqkWew-"
      },
      "source": [
        "And also modify this code, so that mem copies from CPU to GPU and from GPU to CPU use an stream, and are not blocking.\n",
        "\n",
        "Additionally, add a chrono between the first memcpy (included) and the cudaStreamSynchronize. This is the time you will have to compare.\n",
        "\n",
        "Follow the indications in the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih1ADZpjWeFk"
      },
      "source": [
        "%%cuda --name experiment.h\n",
        "#include <cuda.h>\n",
        "#include <chrono>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "#include \"/content/src/check_results.h\"\n",
        "#include \"/content/src/cuda_launcher.h\"\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "void executeExperiment() {\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  int bar_widht = HEIGHT/3;\n",
        "\n",
        "  // Alloc and generate BRG bars.\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  for (int i=0; i < WIDTH * HEIGHT; ++i) {\n",
        "    if (i < bar_widht) {\n",
        "      uchar3 temp = {255, 0, 0};\n",
        "      h_brg[i] = temp; \n",
        "    } else if (i < bar_widht*2) {\n",
        "      uchar3 temp = {0, 255, 0};\n",
        "      h_brg[i] = temp;\n",
        "    } else { \n",
        "      uchar3 temp = {0, 0, 255};\n",
        "      h_brg[i] = temp;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Alloc RGBA pointers\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  // Alloc gpu pointers\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "  \n",
        "  // Start measuring time here\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba);\n",
        "\n",
        "  // Execute the GPU kernel\n",
        "  executeKernelconvertBRG2RGBA(WIDTH, HEIGHT, d_brg, d_rgba, EXPERIMENT_ITERATIONS);\n",
        "\n",
        "  // Copy data back from GPU to CPU\n",
        "  CU_CHECK2(cudaErrorInvalidValue, \"Cuda memcpy Device to Host: \");\n",
        "\n",
        "  // Synchronize the stream here\n",
        "  // Stop measuring time here, and print it\n",
        "    \n",
        "  // Check results\n",
        "  bool ok = checkResults(h_rgba, h_brg, WIDTH*HEIGHT);\n",
        "  if (ok) {\n",
        "      std::cout << \"Executed!! Results OK.\" << std::endl;\n",
        "  } else {\n",
        "      std::cout << \"Executed!! Results NOT OK.\" << std::endl;\n",
        "  }\n",
        "\n",
        "  // Free CPU pointers\n",
        "  free(h_rgba);\n",
        "  free(h_brg);\n",
        "\n",
        "  // Free cuda pointers\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cb3WvwUW6E7"
      },
      "source": [
        "Do the following:\n",
        "\n",
        "1.   Use the fastest kernel version.\n",
        "2.   Use number of iterations = 1.\n",
        "3.   Compare the same kernel, with the original Host code, and this new Host code.\n",
        "4.   To do so, you can use the code you do now, you only need to set stream=0 in order to simulate the original code.\n",
        "5.   Execute it with the following code.\n",
        "6.   Compare and try to explain the performance difference in the report.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icJq5yuCW9f_"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}